Query Formulation:

The user inputs a question or prompt.

Optionally, this input may be reformulated or expanded (e.g., with query rewriting techniques).

Embedding the Query:

The query is transformed into a dense vector using a pre-trained embedding model (e.g., sentence transformers, BERT, etc.).

This allows the system to perform semantic search, not just keyword matching.

Document Retrieval:

The query vector is compared with vectors of documents in a vector database (e.g., FAISS, Milvus, Pinecone).

The top-k most relevant documents (chunks/passages) are retrieved based on similarity (usually cosine similarity).

Optional: Re-ranking:

Retrieved documents may be re-ranked using a more accurate (but slower) model, like a cross-encoder.

This improves the relevance of final documents passed to the generator.

Passing to Generator (LLM):

The retrieved documents are fed into a language model (e.g., GPT, T5, etc.) along with the original question.

The LLM uses both the question and retrieved context to generate a more informed and accurate response.